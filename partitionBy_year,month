Write output to ADLS in partitioned Parquet by year and month

from pyspark.sql.types import * 
from pyspark.sql.functions import * 
from datetime import datetime 
from zoneinfo import ZoneInfo 
from pyspark.sql.functions import col, current_date 

data = [("Monica",25,50000,"DataEngineer","2025-02-02"),("Pragathi",31,100000,"JavaDeveloper","2024-01-20"),("Veera",32,1500000,"JavaDeveloper","2026-02-01"),
("Shiva",25,100000,"DataEngineer","2026-02-07"),("sathya",25,899999,"JavaDeveloper","2024-02-01")] 
schema = ["name","age","salary","department","doj"] 

df = spark.createDataFrame(data,schema) 
df = df.withColumn("doj",to_date(col("doj"),"yyyy-MM-dd")) 
df = df.withColumn("year", year(col("doj"))).withColumn("month",month(col("doj"))) 
display(df) 

path = dbutils.secrets.get(scope ="foa-kv-scope" ,key = "foa-mount-path") + "/inbound/test/" 
df.write.format("csv").mode("append").partitionBy("year","month").save(path)
